{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Implementation for the Toxic Comment Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T07:47:54.690332Z",
     "start_time": "2018-05-01T07:47:54.681534Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T07:53:02.291924Z",
     "start_time": "2018-05-01T07:53:02.287784Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"../../../data/wikipedia/train.csv\"\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "comment_col = 'comment_text'\n",
    "target_names = ['Predicted negative', 'Predicted positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T07:48:26.261114Z",
     "start_time": "2018-05-01T07:48:25.477459Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T07:48:26.695064Z",
     "start_time": "2018-05-01T07:48:26.690941Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_hot_encoding(X):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder = label_encoder.fit(X)\n",
    "    label_encoded_x = label_encoder.transform(X)\n",
    "    return label_encoded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T07:55:57.949822Z",
     "start_time": "2018-05-01T07:55:47.040776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Predicted negative       0.91      1.00      0.95     28836\n",
      "Predicted positive       0.79      0.06      0.12      3079\n",
      "\n",
      "       avg / total       0.90      0.91      0.87     31915\n",
      "\n",
      "\n",
      "Accuracy with label toxic 90.79743067523108\n",
      "\n",
      "AUC with label toxic 0.5306020842645045\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Predicted negative       0.99      1.00      0.99     31587\n",
      "Predicted positive       0.00      0.00      0.00       328\n",
      "\n",
      "       avg / total       0.98      0.99      0.98     31915\n",
      "\n",
      "\n",
      "Accuracy with label severe_toxic 98.97227009243302\n",
      "\n",
      "AUC with label severe_toxic 0.5\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Predicted negative       0.95      1.00      0.97     30228\n",
      "Predicted positive       0.98      0.07      0.13      1687\n",
      "\n",
      "       avg / total       0.95      0.95      0.93     31915\n",
      "\n",
      "\n",
      "Accuracy with label obscene 95.07441641861195\n",
      "\n",
      "AUC with label obscene 0.5346438594051343\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Predicted negative       1.00      1.00      1.00     31832\n",
      "Predicted positive       0.00      0.00      0.00        83\n",
      "\n",
      "       avg / total       0.99      1.00      1.00     31915\n",
      "\n",
      "\n",
      "Accuracy with label threat 99.73993420021934\n",
      "\n",
      "AUC with label threat 0.5\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Predicted negative       0.95      1.00      0.97     30289\n",
      "Predicted positive       0.81      0.05      0.09      1626\n",
      "\n",
      "       avg / total       0.94      0.95      0.93     31915\n",
      "\n",
      "\n",
      "Accuracy with label insult 95.0963496788344\n",
      "\n",
      "AUC with label insult 0.5242866007847242\n",
      "-------------------------------------------------------\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Predicted negative       0.99      1.00      1.00     31641\n",
      "Predicted positive       0.00      0.00      0.00       274\n",
      "\n",
      "       avg / total       0.98      0.99      0.99     31915\n",
      "\n",
      "\n",
      "Accuracy with label identity_hate 99.1414695284349\n",
      "\n",
      "AUC with label identity_hate 0.5\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X = text_hot_encoding(train[comment_col])\n",
    "accuracy_dict = dict()\n",
    "seed = 7\n",
    "test_size = 0.2\n",
    "\n",
    "for i in range(len(label_cols)):\n",
    "    Y = train[label_cols[i]]\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    \n",
    "    # fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train[:,np.newaxis],y_train)\n",
    "    \n",
    "    #testing with X_test data\n",
    "    #X_test = text_hot_encoding(X_test)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test[:,np.newaxis])\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions, target_names=target_names))\n",
    "    \n",
    "    print(\"\\nAccuracy with label\", label_cols[i], str(accuracy * 100.0))\n",
    "    print(\"\\nAUC with label\", label_cols[i], str(auc))\n",
    "    print(\"-------------------------------------------------------\\n\")\n",
    "    \n",
    "    accuracy_dict[label_cols[i]] = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
